from datasets import load_dataset
from nomic import AtlasDataset
import numpy as np
import hashlib
from PIL import Image
import base64
import io


# Converts images to base64
def image_to_base64(image):
    buffered = io.BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")


# Flattens nested dictionaries and lists into a dictionary
def flatten_data(entry):
    flat_entry = {}
    for key, value in entry.items():
        if isinstance(value, dict):
            for sub_key, sub_value in flatten_data(value).items():
                flat_entry[f"{key}_{sub_key}"] = sub_value
        elif isinstance(value, list):
            if all(isinstance(i, dict) for i in value):
                for idx, item in enumerate(value):
                    for sub_key, sub_value in flatten_data(item).items():
                        flat_entry[f"{key}_{idx}_{sub_key}"] = sub_value
            else:
                flat_entry[key] = ' '.join(map(str, value))
        else:
            flat_entry[key] = value
    return flat_entry


# Gets data from a Hugging Face dataset with auto config
def fetch_data_from_huggingface(dataset_identifier):
    try:
        # Try loading the dataset without specifying a configuration
        dataset = load_dataset(dataset_identifier)
    except ValueError as e:
        # If there is an error it might be because of the config selection
        if "Please pick one among the available configs" in str(e):
            # Gets available config and selects first one
            available_configs = str(e).split("['")[1].split("']")[0].split("', '")
            dataset = load_dataset(dataset_identifier, available_configs[0])
        else:
            raise e

    data = []
    for split in dataset.keys():
        for i, example in enumerate(dataset[split]):
            # Creates a unique and shortened ID
            unique_str = f"{dataset_identifier}_{split}_{i}"
            short_id = hashlib.sha1(unique_str.encode()).hexdigest()[:25]
            example['id'] = short_id
            data.append(flatten_data(example))
    return data


# Preprocesses field entries: flattens lists, converts arrays to strings, or converts to string
def preprocess_field_entries(data, indexed_field):
    for entry in data:
        if isinstance(entry[indexed_field], list):
            entry[indexed_field] = ' '.join(map(str, entry[indexed_field]))
        elif isinstance(entry[indexed_field], np.ndarray):
            entry[indexed_field] = ' '.join(map(str, entry[indexed_field].flatten()))
        elif hasattr(entry[indexed_field], 'tolist'):
            entry[indexed_field] = ' '.join(map(str, entry[indexed_field].tolist()))
        elif isinstance(entry[indexed_field], (str, bool, int, float)):
            entry[indexed_field] = str(entry[indexed_field])
        elif isinstance(entry[indexed_field], Image.Image):
            entry[indexed_field] = image_to_base64(entry[indexed_field])
        else:
            entry[indexed_field] = str(entry[indexed_field])
    return data


# Converts all boolean fields and list fields in the dataset to strings
def convert_booleans_and_lists_to_strings(data):
    for entry in data:
        for key, value in entry.items():
            if isinstance(value, bool):
                entry[key] = str(value)
            elif isinstance(value, list):
                entry[key] = ' '.join(map(str, value))
            elif isinstance(value, Image.Image):
                entry[key] = image_to_base64(value)
            elif isinstance(value, str):
                entry[key] = value 
    return data


# Creates map from a list of data entries
def create_map_from_data(data, map_name, indexed_field):
    if not data:
        raise ValueError("No data found.")

    # Convert all boolean fields and list fields to strings
    data = convert_booleans_and_lists_to_strings(data)

    # Check if indexed_field exists in data
    if indexed_field not in data[0]:
        raise ValueError(f"Field '{indexed_field}' not found in the dataset.")

    # Preprocess field entries
    data = preprocess_field_entries(data, indexed_field)

    dataset = AtlasDataset(
        map_name,
        unique_id_field="id",
    )
    dataset.add_data(data=data)

    map = dataset.create_index(
        indexed_field=indexed_field,
        topic_model=True,
        embedding_model='NomicEmbed'
    )

    return map


if __name__ == "__main__":
    dataset_identifiers = input("Enter Hugging Face dataset identifiers separated by commas: ").split(',')
    indexed_field = input("Enter the field to index by (e.g., 'text'): ").strip()
    all_data = []
    dataset_names = []

    for dataset_identifier in dataset_identifiers:
        dataset_name = dataset_identifier.split('/')[-1].strip()
        dataset_names.append(dataset_name)

        data = fetch_data_from_huggingface(dataset_identifier.strip())
        if data:
            all_data.extend(data)

    if all_data:
        combined_dataset_names = '_'.join(dataset_names)
        map_name = combined_dataset_names

        try:
            data_map = create_map_from_data(all_data, map_name, indexed_field)
            print(f"Data map '{map_name}' has been created")
        except ValueError as e:
            print(f"Error creating data map: {e}")
    else:
        print("No data was found for the provided datasets.")



